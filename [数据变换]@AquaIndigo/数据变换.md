# 数据变换和一些其他非监督模型

在`MLJ`中有几个非监督模型可用于通常的数据变换，例如 one-hot 编码。

`MLJ`的变换器（transformer）如果没有学习参数，则是*静态*的，这样的转换器相当于一个普通的函数。我们得理解这是个静态的`MLJ`变换器（虽然我也没有具体理解），尤其是在函数依赖用户控制的一些参数（模型中的超参数）。（这一部分笔者还没有搞清楚具体是什么东西，参见第二节静态转换）

一些非监督模型，例如聚类算法，除了`transform`方法外还具有`predict`方法，在[可预测的转换器](##可预测的转换器)给出了中一些例子。

最后注意将一个分布或更广泛的样本变换到一些其他数据的模型是有监督的模型，虽然有时把它们看做无监督模型。[Models that learn a probability](https://alan-turing-institute.github.io/MLJ.jl/stable/adding_models_for_general_use/#Models-that-learn-a-probability-distribution-1)中有一些例子。

前提知识：[科学类型](https://alan-turing-institute.github.io/MLJScientificTypes.jl/dev/)，`MLJ`包引入了一个重要的概念——科学类型，这个是应用模型的基础，不同的模型对应着不同的输入、输出类型。下面列举了一部分科学类型和其继承关系，其中第一层是抽象类型（如`Infinite`）。

```julia
Finite{N}
├─ Multiclass{N}
└─ OrderedFactor{N}

Infinite
├─ Continuous
└─ Count

Image{W,H}
├─ ColorImage{W,H}
└─ GrayImage{W,H}

ScientificTimeType
├─ ScientificDate
├─ ScientificTime
└─ ScientificDateTime
```

下面是一部分我们熟知的类型和`MLJ`科学类型的转换关系：

| Type `T`            | `scitype(x)` for `x::T`                                      | package required  |
| :------------------ | :----------------------------------------------------------- | :---------------- |
| `Missing`           | `Missing`                                                    |                   |
| `AbstractFloat`     | `Continuous`                                                 |                   |
| `Integer`           | `Count`                                                      |                   |
| `String`            | `Textual`                                                    |                   |
| `CategoricalValue`  | `Multiclass{N}` where `N = nlevels(x)`, provided `x.pool.ordered == false` | CategoricalArrays |
| `CategoricalString` | `Multiclass{N}` where `N = nlevels(x)`, provided `x.pool.ordered == false` | CategoricalArrays |
| `CategoricalValue`  | `OrderedFactor{N}` where `N = nlevels(x)`, provided `x.pool.ordered == true` | CategoricalArrays |
| `CategoricalString` | `OrderedFactor{N}` where `N = nlevels(x)` provided `x.pool.ordered == true` | CategoricalArrays |

全部转换关系见[Summary of the MLJ convention](https://alan-turing-institute.github.io/MLJScientificTypes.jl/dev/#Summary-of-the-MLJ-convention-1)中的描述。

较为常用的科学类型：

- `Continuous`连续型，与`Count`同属于无穷类，各种`Float`类型均是

- `Count`，各种整数类，一般模型会要求类型是`Infinite`，或者只接受`Continuous`，所以可能需要类型转换，方式在转换器部分

- `Multiclass{N}`和`OrderedFactor{N}`（`N`代表类别数），都是分类型数据，不同之处是前者没有具体的次序之分，一般用one-hot编码替代，后者有次序，例如成绩的A、B、C档次等等。

  one-hot编码指的是，用一个`N`维向量（可以缩减到`N-1`维）来表示一个类别，每个类别有一个维度是`1`，其他均为`0`，由于每个维度是相互独立的，所以不存在次序之分。例如有三个品种可以分别用`[1, 0, 0]、[0, 1, 0]、[0, 0, 1]`表示。

## 内置转换器

#### `MLJModels.UnivariateStandardizer` 

将单变量（向量）标准化，效果相当于计算
$$
\frac{x - \bar x}{Std(x)}
$$
用法

```julia
julia> using MLJModels, Statistics, MLJ

julia> ustandr = UnivariateStandardizer();

julia> x = rand(10);

julia> usdMC = machine(ustandr, x)
Machine{UnivariateStandardizer} @ 5…94


julia> fit!(usdMC);
[ Info: Training Machine{UnivariateStandardizer} @ 5…94.

julia> xsd = transform(usdMC, x);

julia> xsd == (x .- mean(x)) ./ std(x)
true
    
julia> xsd == transform(fit!(machine(UnivariateStandardizer(), x)), x); # 简写
[ Info: Training Machine{UnivariateStandardizer} @ 1…19.
true
    
```

可以看出，这个转换器的效果和 `(x .- mean(x)) ./ std(x)`是一致的，都是将样本标准化为均值为`0`，方差为 `1` 的分布（高斯分布多用这样的转换）。

#### `MLJModels.Standardizer`

`Standardizer(; features=Symbol[], ignore=false, ordered_factor=false, count=false)`

用来标准化表格状数据的每一列的非监督模型（表格状的明明元组，或者`DataFrame`等等），如果不指定`features`，每个元素的scitype为`Continuous`（例如浮点数）的列均会被标准化；否则，如果`ignore=false`，在`features`中`Continuous` 的那些列将会被转换，`ignore=true`，没有被选中的那些features将会被标准化。若要支持`Count`（如`Int`类型）或者 `OrderedFactor`类型的转换，则需将相应的最后两个flag设置为`true`。

`features`除了接收表示特征的向量，还能接收返回布尔值可调用对象（例子中是匿名函数或者），例如`Standardizer(features = name -> name in [:x1, :x3], ignore = true, count=true)` 和 `Standardizer(features = [:x1, :x3], ignore = true, count=true)`效果是一样的，意味着标准化所有除了`:x1`或`:x3`中`Continuous`  和 `Count`特征。

Examples:

```julia
julia> using MLJModels, CategoricalArrays, MLJBase

julia> X = (ordinal1 = [1, 2, 3],
            ordinal2 = categorical([:x, :y, :x], ordered=true),
            ordinal3 = [10.0, 20.0, 30.0],
            ordinal4 = [-20.0, -30.0, -40.0],
            nominal = categorical(["Your father", "he", "is"]));#命名元组

julia> stand1 = Standardizer();

julia> transform(fit!(machine(stand1, X)), X)
[ Info: Training Machine{Standardizer} @ 7…97.
(ordinal1 = [1, 2, 3],
 ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],
 ordinal3 = [-1.0, 0.0, 1.0],
 ordinal4 = [1.0, 0.0, -1.0],
 nominal = CategoricalVale{String,UInt32}["Your father", "he", "is"],)

julia> stand2 = Standardizer(features=[:ordinal3, ], ignore=true, count=true);#:ordinal3 不被标准化

julia> transform(fit!(machine(stand2, X)), X) 
[ Info: Training Machine{Standardizer} @ 1…87.
(ordinal1 = [-1.0, 0.0, 1.0],
 ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],
 ordinal3 = [10.0, 20.0, 30.0],
 ordinal4 = [1.0, 0.0, -1.0],
 nominal = CategoricalValue{String,UInt32}["Your father", "he", "is"],)
        
julia> X = DataFrame(X);

julia> transform(fit!(machine(stand1, X)), X) #Count (Int)没有被标准化
[ Info: Training Machine{Standardizer} @ 1…66.
3×5 DataFrame
│ Row │ ordinal1 │ ordinal2 │ ordinal3 │ ordinal4 │ nominal     │
│     │ Int64    │ Cat…     │ Float64  │ Float64  │ Cat…        │
├─────┼──────────┼──────────┼──────────┼──────────┼─────────────┤
│ 1   │ 1        │ :x       │ -1.0     │ 1.0      │ Your father │
│ 2   │ 2        │ :y       │ 0.0      │ 0.0      │ he          │
│ 3   │ 3        │ :x       │ 1.0      │ -1.0     │ is          │

julia> transform(fit!(machine(Standardizer(count=true), X)), X)
[ Info: Training Machine{Standardizer} @ 1…24.
3×5 DataFrame
│ Row │ ordinal1 │ ordinal2 │ ordinal3 │ ordinal4 │ nominal     │
│     │ Float64  │ Cat…     │ Float64  │ Float64  │ Cat…        │
├─────┼──────────┼──────────┼──────────┼──────────┼─────────────┤
│ 1   │ -1.0     │ :x       │ -1.0     │ 1.0      │ Your father │
│ 2   │ 0.0      │ :y       │ 0.0      │ 0.0      │ he          │
│ 3   │ 1.0      │ :x       │ 1.0      │ -1.0     │ is          │
```



#### `MLJModels.OneHotEncoder`

```julia
OneHotEncoder(; features=Symbol[],
                ignore=false,
                ordered_factor=true,
                drop_last=false)
```

对`Finite`特征进行 one-hot （一位有效）编码。参数`features`与`ignore`的作用与上文类似。	

若`ordered_factor=false`，上述对 `Finite` 的规则改为对 `Multiclass`成立，即对 `OrderedFactor`不会进行转换。

若指定`drop_last=true`，每个特征的最后一个等级的列会被丢弃。

Examples:

```julia
X = (name=categorical(["Danesh", "Lee", "Mary", "John"]),
     grade=categorical([:A, :B, :A, :C], ordered=true),
     height=[1.85, 1.67, 1.5, 1.67],
     n_devices=[3, 2, 4, 3])
schema(X)

┌───────────┬─────────────────────────────────┬──────────────────┐
│ _.names   │ _.types                         │ _.scitypes       │
├───────────┼─────────────────────────────────┼──────────────────┤
│ name      │ CategoricalValue{String,UInt32} │ Multiclass{4}    │
│ grade     │ CategoricalValue{Symbol,UInt32} │ OrderedFactor{3} │
│ height    │ Float64                         │ Continuous       │
│ n_devices │ Int64                           │ Count            │
└───────────┴─────────────────────────────────┴──────────────────┘
_.nrows = 4

hot = OneHotEncoder(ordered_factor=true); #name and grade will both be transformed
mach = fit!(machine(hot, X))
transform(mach, X) |> schema

┌──────────────┬─────────┬────────────┐
│ _.names      │ _.types │ _.scitypes │
├──────────────┼─────────┼────────────┤
│ name__Danesh │ Float64 │ Continuous │
│ name__John   │ Float64 │ Continuous │
│ name__Lee    │ Float64 │ Continuous │
│ name__Mary   │ Float64 │ Continuous │
│ grade__A     │ Float64 │ Continuous │
│ grade__B     │ Float64 │ Continuous │
│ grade__C     │ Float64 │ Continuous │
│ height       │ Float64 │ Continuous │
│ n_devices    │ Int64   │ Count      │
└──────────────┴─────────┴────────────┘
_.nrows = 4
```



#### `MLJModels.ContinuousEncoder`

用来将所有列重构成 `Continuous` 类型的非监督模型，对每个特征（feature）`ftr`，通过以下方式变换：

- If `ftr` is already `Continuous` 保持.
- If `ftr` is `Multiclass`, one-hot encode it.
- If `ftr` is `OrderedFactor`,用 `coerce(ftr, Continuous)` 进行变换（参见[MLJScientificTypes.coerce](https://alan-turing-institute.github.io/MLJScientificTypes.jl/dev/#MLJScientificTypes.coerce) ）。或者指定`one_hot_ordered_factors=false`，此时
- If `ftr` is `Count`, replace it with `coerce(ftr, Continuous)`.
- If `ftr` is of some other element scitype, or was not observed in fitting the encoder，将会被丢弃

若 `drop_last=true`，最后一个 one-hot encoding 将被丢弃。

**Example**:

```julia
X = (name=categorical(["Danesh", "Lee", "Mary", "John"]),
     grade=categorical([:A, :B, :A, :C], ordered=true),
     height=[1.85, 1.67, 1.5, 1.67],
     n_devices=[3, 2, 4, 3],
     comments=["the force", "be", "with you", "too"])
schema(X)

┌───────────┬─────────────────────────────────┬──────────────────┐
│ _.names   │ _.types                         │ _.scitypes       │
├───────────┼─────────────────────────────────┼──────────────────┤
│ name      │ CategoricalValue{String,UInt32} │ Multiclass{4}    │
│ grade     │ CategoricalValue{Symbol,UInt32} │ OrderedFactor{3} │
│ height    │ Float64                         │ Continuous       │
│ n_devices │ Int64                           │ Count            │
│ comments  │ String                          │ Textual          │
└───────────┴─────────────────────────────────┴──────────────────┘
_.nrows = 4

cont = ContinuousEncoder(drop_last=true);
mach = fit!(machine(cont, X))
transform(mach, X) |> schema #:comments and  "Mary" were dropped

┌──────────────┬─────────┬────────────┐
│ _.names      │ _.types │ _.scitypes │
├──────────────┼─────────┼────────────┤
│ name__Danesh │ Float64 │ Continuous │
│ name__John   │ Float64 │ Continuous │
│ name__Lee    │ Float64 │ Continuous │
│ grade        │ Float64 │ Continuous │
│ height       │ Float64 │ Continuous │
│ n_devices    │ Float64 │ Continuous │
└──────────────┴─────────┴────────────┘
_.nrows = 4

print(transform(mach, X).grade)
[1.0, 2.0, 1.0, 3.0]
```



#### `MLJModels.FeatureSelector`

```julia
FeatureSelector(features=Symbol[])
```

An unsupervised model for filtering features (columns) of a table. Only those features encountered during fitting will appear in transformed tables if `features` is empty (the default). Alternatively, if a non-empty `features` is specified, then only the specified features are used. Throws an error if a recorded or specified feature is not present in the transformation input.



#### `MLJModels.UnivariateBoxCoxTransformer`

没看明白，学了再添加

BoxCox变换

```julia
UnivariateBoxCoxTransformer(; n=171, shift=false)
```

Unsupervised model specifying a univariate Box-Cox transformation of a single variable taking non-negative values, with a possible preliminary shift. Such a transformation is of the form

```none
x -> ((x + c)^λ - 1)/λ for λ not 0
x -> log(x + c) for λ = 0
```

On fitting to data `n` different values of the Box-Cox exponent λ (between `-0.4` and `3`) are searched to fix the value maximizing normality. If `shift=true` and zero values are encountered in the data then the transformation sought includes a preliminary positive shift `c` of `0.2` times the data mean. If there are no zero values, then no shift is applied.



#### `MLJModels.UnivariateDiscretizer`

```julia
UnivariateDiscretizer(n_classes=512)
```

返回一个用来离散化任意连续型向量`v`的`MLJModel` (`scitype(v) <: AbstractVector{Continuous}`)， `n_classes` 表示离散化的分辨率。

Transformed output `w` is a vector of ordered factors (`scitype(w) <: AbstractVector{<:OrderedFactor}`). Specifically, `w` is a `CategoricalVector`, with element type `CategoricalValue{R,R}`, where `R is optimized.

变换的结果是一个元素是有序因子的向量`w` (`scitype(w) <: AbstractVector{<:OrderedFactor}`)，特别地，最佳的结果是`w` 是 `CategoricalValue{R,R}`类型，且`R<Unsigned` 。

转换后有近似均匀分布的数值。

**Example**

```none
using MLJ
t = UnivariateDiscretizer(n_classes=10)
discretizer = machine(t, randn(1000))
fit!(discretizer)
v = rand(10) 
w = transform(discretizer, v)
v_approx = inverse_transform(discretizer, w) # reconstruction of v from w
```



#### `MLJModels.FillImputer`

```julia
FillImputer(
 features        = [],
 continuous_fill = e -> skipmissing(e) |> median
 count_fill      = e -> skipmissing(e) |> (f -> round(eltype(f), median(f))
 finite_fill     = e -> skipmissing(e) |> mode)
```

Imputes missing data with a fixed value computed on the non-missing values. A different imputing function can be specified for `Continuous`, `Count` and `Finite` data.

用非缺失值来填充缺失值，不同的数据类型会用处理方式进行填充

**Fields**

- `continuous_fill`: 默认median中位数
- `count_fill`: 默认四舍五入后的中位数rounded median
- `finite_fill`: function to use on `Multiclass` and `OrderedFactor` data (including binary data), 默认mode函数（似乎是频率最高的那个量）



## 静态转换

静态转换的常用方法是插入一个 [`@pipeline`](https://alan-turing-institute.github.io/MLJ.jl/stable/composing_models/#MLJBase.@pipeline) 或者其他的输出学习网络 (参见 [Composing Models](https://alan-turing-institute.github.io/MLJ.jl/stable/composing_models/#Composing-Models-1))。如果没有超参数，静态转换跟一个普通的函数等价（参见 [Static operations on nodes](https://alan-turing-institute.github.io/MLJ.jl/stable/composing_models/#Static-operations-on-nodes-1).）。

以下的例子中定义了一个新的模型类型 `Averager` 来计算两个向量的加权平均数，这里假设权重是标准化的（和为1），且有一个超参数`mix`来调控。

```julia
mutable struct Averager <: Static
    mix::Float64
end

import MLJBase
MLJBase.transform(a::Averager, _, y1, y2) = (1 - a.mix)*y1 + a.mix*y2
```

*特别要*注意，这是个`<: Static`的子类型。

Such static transformers with (unlearned) parameters can have arbitrarily many inputs, but only one output. In the single input case an `inverse_transform` can also be defined. Since they have no real learned parameters, you bind a static transformer to a machine without specifying training arguments.

```julia
mach = machine(Averager(0.5)) |> fit!
transform(mach, [1, 2, 3], [3, 2, 1])
3-element Array{Float64,1}:
 2.0
 2.0
 2.0
```

Let's see how we can include our `Averager` in a learning network (see [Composing Models](https://alan-turing-institute.github.io/MLJ.jl/stable/composing_models/#Composing-Models-1)) to mix the predictions of two regressors, with one-hot encoding of the inputs:

下面展示如何将 `Averager` 引入一个学习网络来混合两个返回 one-hot 编码的回归器的预测值

```julia
X = source()
y = source(kind=:target)

ridge = @load RidgeRegressor pkg=MultivariateStats
knn = @load KNNRegressor
averager = Averager(0.5)

hotM = machine(OneHotEncoder(), X)
W = transform(hotM, X) # one-hot encode the input

ridgeM = machine(ridge, W, y)
y1 = predict(ridgeM, W)

knnM = machine(knn, W, y)
y2 = predict(knnM, W)

mach = machine(averager)
yhat = transform(mach, y1, y2)
```

输出模型来获得一个复合模型的实例

```julia
composite = @from_network(DoubleRegressor(regressor1=ridge,
                                          regressor2=knn,
                                          averager=averager) <= yhat)
DoubleRegressor(
    regressor1 = RidgeRegressor(
            lambda = 1.0),
    regressor2 = KNNRegressor(
            K = 5,
            algorithm = :kdtree,
            metric = Distances.Euclidean(0.0),
            leafsize = 10,
            reorder = true,
            weights = :uniform),
    averager = Averager(
            mix = 0.5)) @ 1…66
```

which can be can be evaluated like any other model:

这个混合模型能像其他模型一样金星评估

```julia
composite.averager.mix = 0.25 # adjust mix from default of 0.5
evaluate(composite, (@load_reduced_ames)..., measure=rms)
julia> evaluate(composite, (@load_reduced_ames)..., measure=rms)
Evaluating over 6 folds: 100%[=========================] Time: 0:00:00
┌───────────┬───────────────┬────────────────────────────────────────────────────────┐
│ _.measure │ _.measurement │ _.per_fold                                             │
├───────────┼───────────────┼────────────────────────────────────────────────────────┤
│ rms       │ 26800.0       │ [21400.0, 23700.0, 26800.0, 25900.0, 30800.0, 30700.0] │
└───────────┴───────────────┴────────────────────────────────────────────────────────┘
_.per_observation = [missing]
```

## 可预测的转换器

Commonly, clustering algorithms learn to label data by identifying a collection of "centroids" in the training data. Any new input observation is labeled with the cluster to which it is closest (this is the output of `predict`) while the vector of all distances from the centroids defines a lower-dimensional representation of the observation (the output of `transform`). In the following example a K-means clustering algorithm assigns one of three labels 1, 2, 3 to the input features of the iris data set and compares them with the actual species recorded in the target (not seen by the algorithm).

通常聚类算法通过识别训练数据的一系列“中心点”来学习标签数据。聚类器会根据与新的输入最近的中心进行打标签（`predict`的结果）。而这个距离是由 `transform` 给出的。

以下是对`iris`数据集进行的K-means聚类算法，标签对算法是不可见的。

```julia
import Random.seed!
seed!(123)

X, y = @load_iris;
model = @load KMeans pkg=ParallelKMeans
mach = machine(model, X) |> fit!

# transforming:
Xsmall = transform(mach);
selectrows(Xsmall, 1:4) |> pretty
julia> selectrows(Xsmall, 1:4) |> pretty
┌─────────────────────┬────────────────────┬────────────────────┐
│ x1                  │ x2                 │ x3                 │
│ Float64             │ Float64            │ Float64            │
│ Continuous          │ Continuous         │ Continuous         │
├─────────────────────┼────────────────────┼────────────────────┤
│ 0.0215920000000267  │ 25.314260355029603 │ 11.645232464391299 │
│ 0.19199200000001326 │ 25.882721893491123 │ 11.489658693899486 │
│ 0.1699920000000077  │ 27.58656804733728  │ 12.674412792260142 │
│ 0.26919199999998966 │ 26.28656804733727  │ 11.64392098898145  │
└─────────────────────┴────────────────────┴────────────────────┘

# predicting:
yhat = predict(mach);
compare = zip(yhat, y) |> collect;
compare[1:8]
8-element Array{Tuple{CategoricalValue{Int64,UInt32},CategoricalString{UInt32}},1}:
 (1, "setosa")
 (1, "setosa")
 (1, "setosa")
 (1, "setosa")
 (1, "setosa")
 (1, "setosa")
 (1, "setosa")
 (1, "setosa")

compare[51:58]
8-element Array{Tuple{CategoricalValue{Int64,UInt32},CategoricalString{UInt32}},1}:
 (2, "versicolor")
 (3, "versicolor")
 (2, "versicolor")
 (3, "versicolor")
 (3, "versicolor")
 (3, "versicolor")
 (3, "versicolor")
 (3, "versicolor")

compare[101:108]
8-element Array{Tuple{CategoricalValue{Int64,UInt32},CategoricalString{UInt32}},1}:
 (2, "virginica")
 (3, "virginica")
 (2, "virginica")
 (2, "virginica")
 (2, "virginica")
 (2, "virginica")
 (3, "virginica")
 (2, "virginica")
```